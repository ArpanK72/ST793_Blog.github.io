<hr>
<style>
body{
text-align: justify
}
</style>

# Introduction

This project studies the paper titled \`\`Linear Hypothesis Testing in
Linear Models With High-Dimensional Responsesâ€ by Li and Li (2022)
(Runze Li 2022) where the authors talk about the high-dimensional
(*p*â‰«*n*) scenarios where Hotelling *T*<sup>2</sup> cannot be used to do
one-sample and two-sample testing because of the singularity of sample
covariance matrix. Although, there have several papers on solving this
problem and its subsequent problems, the paper of our interest develops
general linear hypothesis testing frameworks using projection matrices,
which can be applied to one-sample, two-sample and multiple sample means
testing in high-dimensions. The main goal of this project is to
understand, simplify the concepts and lucidly explain this new
projection test and its theoretical properties through a blog. As we
proceed further in this project, we both shall elucidate the theorems in
the main paper as far as possible.

One sample and two sample tests have gained a lot of focus in the last
two decades. The Hotelling *T*<sup>2</sup> test for the one-sample and
two-sample mean problem is the uniformly most powerful test invariant
under affine transformations for fixed or low-dimensional data. The
singularity of the sample covariance matrix renders the Hotelling
*T*<sup>2</sup> test invalid when the dimension *p* of observations
exceeds the sample size *n*. Numerous tests for high-dimensional one and
two-sample mean testing have been proposed to address the singularity.
When examining the effects of dimensionality on two-sample mean testing,
Bai and Saranadasa (1996) (Bai and Saranadasa 1996) promoted
sum-of-squares-type statistics for the mean difference and disregarded
correlation matrices. Their test circumvents the issue brought on by the
sample covariance matricesâ€™ rank deficiency. There are also various
other tests that have been created to address any potential issues that
may arise in this area. To create the methods, some of them used
supremum-type statistics, U-statistics, and so on. The authors of this
paper offer a projection test for a specific setting. The correlation
information is used by the projection test to increase the testâ€™s power.
They build the projection test by first projecting the data into low
dimension and then performing the test on the projected data. They
demonstrated that there are low-dimensional projection matrices that
leverage covariance information to maximize the power of projection
tests. They further demonstrate that the dimension of the optimal
projections is not more than *m*, and they derive the *m* optimal
projection directions to maximize power. They adopt a sample-splitting
approach to perform projection tests, in which they use the first
portion of the sample for estimate of projection matrices and the second
part for testing. They also suggest U-projection tests to increase the
testâ€™s power based on the sample-splitting technique by creating a
U-type test statistic from all samples. They have also established the
theoretical properties of the U-projection test.

# Notations

Let us consider the classical linear model setup:
*Y*â€„=â€„*X**B*â€…+â€…*E*
where *Y* is an *n*â€…Ã—â€…*p* response matrix, *X* is an *n*â€…Ã—â€…*d* design
matrix whose dimension *d* is fixed, *B* is a *d*â€…Ã—â€…*p* regression
coefficient matrix, and *E* is an *n*â€…Ã—â€…*p* error matrix with mean zero
and covariance matrix *I*<sub>*n*</sub>â€…âŠ—â€…*Î£* for a positive definite
matrix *Î£*. We denote
vec(*A*)â€„=â€„(*a*<sub>1</sub><sup>âŠ¤</sup>,â‹¯,*a*<sub>*n*</sub><sup>âŠ¤</sup>)<sup>âŠ¤</sup>
for
*A*â€„=â€„(*a*<sub>1</sub>,*a*<sub>2</sub>,â‹¯,*a*<sub>*n*</sub>)<sup>âŠ¤</sup>
from here onward. We are interested in testing the following hypotheses:
$$\begin{aligned}
    H_0&: A_0B = 0 \\
    H_1 & : A_0B\ne 0 \notag
\end{aligned}$$
where *A*<sub>0</sub> is an *m*â€…Ã—â€…*d* known constant matrix with both
*m* and *d* fixed and *A*<sub>0</sub> is of full row rank which implies
*m*â€„â‰¤â€„*d*. Under usual assumptions of dimensions, i.e., *n*â€„\<â€„*p*, the
likelihood ratio test (LRT) and its asymptotic convergence to
chi-squared distribution holds good. However, in high dimensions when
*p*â€„\>â€„â€„\>â€„*n*, the usual properties of LRT fails. In this blog, we
elucidate a methods of testing hypotheses in such scenarios using
projection matrices that project the high-dimension problem to low
dimensions such that a tractable solution is achieved using the
conventional LRT.

# Optimal Projection Direction

Firstly, let us redefine the problem. Let *P* be a *p*â€…Ã—â€…*r* full column
rank projection matrix with *r*â€„â‰ªâ€„*p* and *r*â€„\<â€„*n*, consider the
*P*-projected model:

*Y*<sup>â‹†</sup>â€„=â€„*X**B*<sup>â‹†</sup>â€…+â€…*E*<sup>â‹†</sup>

where *Y*<sup>â‹†</sup>â€„=â€„*Y**P*, *B*<sup>â‹†</sup>â€„=â€„*B**P*, and
*E*<sup>â‹†</sup>â€„=â€„*E**P*, which is the *n*â€…Ã—â€…*r* projected error matrix
with mean zero and covâ€†(vec(*E*<sup>â‹†</sup>))â€„=â€„*I*<sub>*n*</sub>âŠ—
(*P*<sup>âŠ¤</sup>*Î£**P*)

The corresponding projected hypothesis becomes

*H*<sub>0*P*</sub>â€„:â€„*A*<sub>0</sub>*B*<sup>â‹†</sup>â€„=â€„0â€Šâ€ versus â€Šâ€*H*<sub>1*P*</sub>â€„:â€„*A*<sub>0</sub>*B*<sup>â‹†</sup>â€„â‰ â€„0

and the projected test statistic is

$$\Lambda=\frac{\left\|G\_{P}\right\|}{\left\|G\_{P}+H\_{P}\right\|}$$

where *G*<sub>*P*</sub> is the residual sum of squares under
*H*<sub>1*P*</sub>, *G*<sub>*P*</sub>â€…+â€…*H*<sub>*P*</sub> is the
residual sum of squares under *H*<sub>0*P*</sub>.
*G*<sub>*P*</sub>â€„=â€„*P*<sup>âŠ¤</sup>*Y*<sup>âŠ¤</sup>(*I*<sub>*n*</sub>âˆ’*P*<sub>*H*<sub>1</sub></sub>)*Y**P*,
*P*<sub>*H*<sub>1</sub></sub>â€„=â€„*X*(*X*<sup>âŠ¤</sup>*X*)<sup>âˆ’1</sup>*X*<sup>âŠ¤</sup>
and
*G*<sub>*P*</sub>â€…+â€…*H*<sub>*P*</sub>â€„=â€„*P*<sup>âŠ¤</sup>*Y*<sup>âŠ¤</sup>(*I*<sub>*n*</sub>âˆ’*P*<sub>*H*<sub>0</sub></sub>)*Y**P*,
*P*<sub>*H*<sub>0</sub></sub>â€„=â€„*X**A*<sub>1</sub>(*A*<sub>1</sub><sup>âŠ¤</sup>*X*<sup>âŠ¤</sup>*X**A*<sub>1</sub>)<sup>âˆ’1</sup>*A*<sub>1</sub><sup>âŠ¤</sup>*X*<sup>âŠ¤</sup>,
and *A*<sub>1</sub> is a *d*â€…Ã—â€…(*d*âˆ’*m*) matrix defined by
*A*<sub>1</sub>â€„=â€„(*A*<sub>0</sub><sup>âŠ¤</sup>)<sup>âŠ¥</sup>, which means
that (*A*<sub>0</sub><sup>âŠ¤</sup>,*A*<sub>1</sub>) forms an orthogonal
matrix. The test is the LRT under normality assumption and is equivalent
to many useful test in various cases, like Hotelling *T*<sup>2</sup> in
the one-sample mean testing.

The one-sample and two-sample mean tests can be written as special
instances of the mentioned hypotheses test if *X* and *A*<sub>0</sub>
are appropriately specified. As a result, the approach suggested in this
report applies directly to the one-sample, two-sample mean and even
multiple-sample mean problems. The projected null hypothesis
*H*<sub>0*P*</sub> is rejected for small *Î›*, and *H*<sub>0</sub> is
rejected if *H*<sub>0*P*</sub> is rejected. In general, *H*<sub>0</sub>
and *H*<sub>0*P*</sub> are not equivalent. In this article, we will show
that there exists an optimal projection direction *P* which makes
*H*<sub>0*P*</sub> equivalent to *H*<sub>0</sub> and also maximizes the
power of the test.

The problem how to construct an optimal direction can be divided into
two sub-problems:

-   to find the dimension *r* of the optimal projection direction *P*,
    and

-   to find the optimal *P* of a particular dimension.

These issues are addressed in Theorem
<a href="#thm1" data-reference-type="ref" data-reference="thm1">1</a>.
We assume the following conditions on the multivariate linear model (1)
to derive the asymptotically optimal projection direction matrix.

**Condition 1**:
âˆƒ*C*<sub>1</sub>â€„\>â€„0â€„âˆ‹â€„âˆ¥*X**X*<sup>âŠ¤</sup>âˆ¥<sub>âˆ</sub>â€„\<â€„*C*<sub>1</sub>,
where
âˆ¥*A*âˆ¥<sub>âˆ</sub>â€„=â€„max<sub>*i*,â€†*j*</sub>\|*a*<sub>*i*,â€†*j*</sub>\| for
*A*â€„=â€„(*a*<sub>*i*,â€†*j*</sub>)<sub>*i*,â€†*j*</sub>.

**Condition 2**: The limit
lim<sub>*n*â€„â†’â€„âˆ</sub>*n*<sup>âˆ’1</sup>(*X*<sup>âŠ¤</sup>*X*)â€„=â€„*M*<sub>*X*</sub>
exists, where *M*<sub>*X*</sub> is a nonsingular *d*â€…Ã—â€…*d* matrix.

**Condition 3**:
âˆƒ*C*<sub>2</sub>â€„\>â€„0â€„âˆ‹â€„ğ”¼(*E*<sub>*i**j*</sub><sup>4</sup>)â€„\<â€„*C*<sub>2</sub>,
*i*â€„=â€„1,â€†â€¦,â€†*n*,â€†*j*â€„=â€„1,â€†â€¦,â€†*p* for elements *E*<sub>*i*,â€†*j*</sub> in
the error matrix.

Conditions 1-3 are quite mild and are used to guarantee asymptotic
normality of least squares estimate in linear models. For example,
Conditions 1 and 2 hold in the one-sample mean testing problem
automatically, and they also hold in the two-sample mean testing problem
if $\frac{n\_{1}}{n\_{1}+n\_{2}} \rightarrow \kappa \in(0,1)$, where
*n*<sub>1</sub> and *n*<sub>2</sub> are sample sizes of the first and
the second samples, respectively.

**Theorem 1** (Optimal Projection Direction). *Suppose that the error
matrix *E* follows a matrix normal distribution. Under Conditions 1-3,
the following statements are valid.*

*(A1) Let
*W*â€„=â€„*Î£*<sup>âˆ’1/2</sup>*B*<sup>âŠ¤</sup>*X*<sup>âŠ¤</sup>(*I*<sub>*n*</sub>âˆ’*P*<sub>*H*<sub>0</sub></sub>)*X**B**Î£*<sup>âˆ’1/2</sup>,
where
*P*<sub>*H*<sub>0</sub></sub>â€„=â€„*X**A*<sub>1</sub>(*A*<sub>1</sub><sup>âŠ¤</sup>*X*<sup>âŠ¤</sup>*X**A*<sub>1</sub>)<sup>âˆ’1</sup>*A*<sub>1</sub><sup>âŠ¤</sup>*X*<sup>âŠ¤</sup>,
and *A*<sub>1</sub> is a *d*Ã— (*d*âˆ’*m*) matrix which is defined by
*A*<sub>1</sub>â€„=â€„(*A*<sub>0</sub><sup>âŠ¤</sup>)<sup>âŠ¥</sup>. *W* is a
*p*â€…Ã—â€…*p* matrix of rank *m*. Suppose *W* admits the eigenvalue
decomposition $W=\sum\_{i=1}^{m} \lambda\_{0 i} R\_{i} R\_{i}^\top$,
where 0â€„\<â€„*Î»*<sub>0*m*</sub>â€„â‰¤â€„â‹¯â€„â‰¤â€„*Î»*<sub>01</sub> are *m* nonzero
eigenvalues, and *R*<sub>*i*</sub>,â€†*i*â€„=â€„1,â€†â€¦,â€†*m*, are the
corresponding orthogonal eigenvectors. Then for any given *r*â€„â‰¤â€„*m*,
*P*<sub>0</sub>â€„=â€„*Î£*<sup>âˆ’1/2</sup>(*R*<sub>1</sub>,â€¦,*R*<sub>*r*</sub>)
is the projection direction matrix which is optimal among all *p*â€…Ã—â€…*r*
matrices for the hypothesis testing problem (2).*

*(A2) The optimal dimension of projection matrix is less than or equal
to *m* for the hypothesis testing problem (2).*

*(A3) Set
*r*â€„=â€„*m*,â€†*Î£*<sup>âˆ’1</sup>*B*<sup>âŠ¤</sup>*A*<sub>0</sub><sup>âŠ¤</sup> is
the projection direction matrix which is optimal among all *p*â€…Ã—â€…*m*
matrices for the hypothesis testing problem (2).*

*(B) Without normality assumption, the statements in (A1)-(A3) are still
valid in asymptotic sense by changing optimal projection to
asymptotically optimal projection which maximizes the asymptotic local
power. *

*Proof.* To prove the theorem, we use the unexplained variability or
variability due to random error components. For the ordinary linear
model, orthogonal split of sum of squares is given by
*Y*<sup>2</sup>â€„=â€„*Y*<sup>*T*</sup>*P*<sub>*H*</sub>*Y*â€…+â€…*Y*<sup>*T*</sup>(*I*<sub>*n*</sub>âˆ’*P*<sub>*H*</sub>)*Y*
where the later term in the summation is accountable for the sum of
squares due to errors.

We first prove Theorem
<a href="#thm1" data-reference-type="ref" data-reference="thm1">1</a>(A).
For linear model and under *H*<sub>1</sub>, the estimated *Y* is the
projection of *Y* onto the column space of *X*, denoted by
*P*<sub>*H*<sub>1</sub></sub>*Y*; and under *H*<sub>0</sub>, the
statement that *A*<sub>0</sub>*B*â€„=â€„0 is equivalent to that *B* is of
the form *A*<sub>1</sub><sup>âŠ¤</sup>*D*, where
(*A*<sub>0</sub>,*A*<sub>1</sub>) forms an orthogonal basis and *D* is a
(*d*âˆ’*m*)â€…Ã—â€…*p* matrix. Hence the estimated *Y* under *H*<sub>0</sub> is
the projection of *Y* onto the column space of
*X**A*<sub>1</sub><sup>âŠ¤</sup>, denoted by
*P*<sub>*H*<sub>0</sub></sub>*Y*. So
*G*â€„=â€„(*Y*âˆ’*P*<sub>*H*<sub>1</sub></sub>*Y*)<sup>âŠ¤</sup>(*Y*âˆ’*P*<sub>*H*<sub>1</sub></sub>*Y*)=
*Y*<sup>âŠ¤</sup>(*I*<sub>*n*</sub>âˆ’*P*<sub>*H*<sub>1</sub></sub>)*Y*,â€†*G*â€…+â€…*H*â€„=â€„(*Y*âˆ’*P*<sub>*H*<sub>0</sub></sub>*Y*)<sup>âŠ¤</sup>(*Y*âˆ’*P*<sub>*H*<sub>0</sub></sub>*Y*)â€„=â€„*Y*<sup>âŠ¤</sup>(*I*<sub>*n*</sub>âˆ’*P*<sub>*H*<sub>0</sub></sub>)*Y*,â€†*H*â€„=â€„*Y*<sup>âŠ¤</sup>(*P*<sub>*H*<sub>1</sub></sub>âˆ’*P*<sub>*H*<sub>0</sub></sub>)*Y*.
Under normality assumption, LRT statistic for *H*<sub>0</sub> defined in
(1) is $\frac{\|G\|}{\|G+H\|}$.

Moreover, in the projection test, *Y* is replaced by *Y**P*. The LRT
statistic in the projection test can be written as
$\frac{\left\|G\_{P}\right\|}{\left\|G\_{P}+H\_{P}\right\|}$, where
*G*<sub>*P*</sub> is the residual sum of squares under
*H*<sub>1*P*</sub>, and *G*<sub>*P*</sub>â€…+â€…*H*<sub>*P*</sub> is the
residual sum of squares under *H*<sub>0*P*</sub>. Without loss of
generality, we assume that *Î£*<sup>1/2</sup>*P* is a projection matrix
*Q*, i.e., *Q* is symmetric and idempotent, so
*P*â€„=â€„*Î£*<sup>âˆ’1/2</sup>*Q* and

*G*<sub>*P*</sub>â€„=â€„*P*<sup>âŠ¤</sup>*Y*<sup>âŠ¤</sup>(*I*<sub>*n*</sub>âˆ’*P*<sub>*H*<sub>1</sub></sub>)*Y**P*â€„=â€„*Q*<sup>âŠ¤</sup>*Î£*<sup>âˆ’1/2</sup>*Y*<sup>âŠ¤</sup>(*I*<sub>*n*</sub>âˆ’*P*<sub>*H*<sub>1</sub></sub>)*Y**Î£*<sup>âˆ’1/2</sup>*Q*

Since
(*I*<sub>*n*</sub>âˆ’*P*<sub>*H*<sub>1</sub></sub>)*Y*â€„âˆ¼â€„*N*(0,(*I*<sub>*n*</sub>âˆ’*P*<sub>*H*<sub>1</sub></sub>)âŠ—*Î£*),
we have
*Î£*<sup>âˆ’1/2</sup>*Y*<sup>âŠ¤</sup>(*I*<sub>*n*</sub>âˆ’*P*<sub>*H*<sub>1</sub></sub>)*Y*â€„âˆ¼â€„*W*<sub>*p*</sub>(*n*âˆ’*d*,*I*<sub>*p*</sub>).
Furthermore, since *Q* is a *p*â€…Ã—â€…*k* projection matrix,
*G*<sub>*P*</sub>â€„=â€„*Q*<sup>âŠ¤</sup>*Î£*<sup>âˆ’1/2</sup>*Y*<sup>âŠ¤</sup>(*I*<sub>*n*</sub>âˆ’*P*<sub>*H*<sub>1</sub></sub>)*Y**Î£*<sup>âˆ’1/2</sup>*Q*âˆ¼
*W*<sub>*k*</sub>(*n*âˆ’*d*,*I*<sub>*k*</sub>). We have
*H*<sub>*P*</sub>â€„=â€„*P*<sup>âŠ¤</sup>*Y*<sup>âŠ¤</sup>(*P*<sub>*H*<sub>1</sub></sub>âˆ’*P*<sub>*H*<sub>0</sub></sub>)*Y**P*â€„=â€„*Q*<sup>âŠ¤</sup>*Î£*<sup>âˆ’1/2</sup>*Y*<sup>âŠ¤</sup>(*P*<sub>*H*<sub>1</sub></sub>âˆ’*P*<sub>*H*<sub>0</sub></sub>)*Y**Î£*<sup>âˆ’1/2</sup>*Q*.
Since
(*P*<sub>*H*<sub>1</sub></sub>âˆ’*P*<sub>*H*<sub>0</sub></sub>)*Y*â€„âˆ¼â€„*N*((*I*<sub>*n*</sub>âˆ’*P*<sub>*H*<sub>0</sub></sub>)*X**B*,(*P*<sub>*H*<sub>1</sub></sub>âˆ’*P*<sub>*H*<sub>0</sub></sub>)âŠ—*Î£*),
we have

*Î£*<sup>1/2</sup>*Y*<sup>âŠ¤</sup>(*P*<sub>*I*<sub>1</sub></sub>âˆ’*P*<sub>*I*<sub>0</sub></sub>)*Y**Î£*<sup>âˆ’1/2</sup>â€„âˆ¼â€„*W*<sub>*p*</sub>(*m*,*I*<sub>*p*</sub>,*Î£*<sup>âˆ’1/2</sup>*B*<sup>âŠ¤</sup>*X*<sup>âŠ¤</sup>(*I*<sub>*n*</sub>âˆ’*P*<sub>*I*<sub>0</sub></sub>)*X**B**Î£*<sup>âˆ’1/2</sup>)

Hence we have

$$\begin{aligned}
H\_{P} & =Q^\top \Sigma^{-1 / 2} Y^\top\left(P\_{H\_{1}}-P\_{H\_{0}}\right) Y \Sigma^{-1 / 2} Q \\
& \sim W\_{k}\left(m, I\_{k}, Q^\top \Sigma^{-1 / 2} B^\top X^\top\left(I\_{n}-P\_{H\_{0}}\right) X B \Sigma^{-1 / 2} Q\right) .
\end{aligned}$$

By Cochranâ€™s Theorem, we know that *G*<sub>*P*</sub> is independent to
*H*<sub>*P*</sub> given *X* and *P*. Proof of Theorem
<a href="#thm1" data-reference-type="ref" data-reference="thm1">1</a>(A1).
Here we prove a more general Theorem
<a href="#thm1" data-reference-type="ref" data-reference="thm1">1</a>(A1)
for any *r*â€„â‰¤â€„*p* by defining *R*<sub>*i*</sub>,â€†*i*â€„=â€„*m*â€…+â€…1,â€†â‹¯,â€†*p*
as orthogonal eigenvectors corresponding to eigenvalue 0 . We first show
that *P*<sub>0</sub>â€„=â€„*Î£*<sup>âˆ’1/2</sup>*Q*<sub>0</sub> is the optimal
*p*â€…Ã—â€…*r* projection direction matrix under normality assumption, where
*Q*<sub>0</sub>â€„=â€„(*R*<sub>1</sub>,â‹¯,*R*<sub>*r*</sub>) and
*R*<sub>*i*</sub> is defined as in Theorem
<a href="#thm1" data-reference-type="ref" data-reference="thm1">1</a> .

Given *r*â€„â‰¤â€„*m* fixed, it is easy to see that
$\lambda\_{P}=\frac{\left\|G\_{P}\right\|}{\left\|G\_{P}+H\_{P}\right\|}$
will be of the same distribution under *H*<sub>0</sub> no matter what
*Q* is, so the critical value will be the same for any *p*â€…Ã—â€…*r*
projection matrix *Q*. So to compare the power of *p*â€…Ã—â€…*r* projection
tests, we only need to compare
$P\left(\frac{\left\|G\_{P}\right\|}{\left\|G\_{P}+H\_{P}\right\|}\<c\right)$
for any positive *c*.

For any *p*â€…Ã—â€…*r* projection matrix *Q*, it is easy to know that
*Î»*<sub>*i*</sub>â€„â‰¤â€„*Î»*<sub>0*i*</sub>, where
*Î»*<sub>*r*</sub>â€„â‰¤â€„*Î»*<sub>*r*â€…âˆ’â€…1</sub>â‰¤ â‹¯â€„â‰¤â€„*Î»*<sub>1</sub> are the
nonzero eigenvalues of
*Q*<sup>âŠ¤</sup>*Î£*<sup>âˆ’1/2</sup>*B*<sup>âŠ¤</sup>*X*<sup>âŠ¤</sup>(*I*<sub>*n*</sub>âˆ’*P*<sub>*H*<sub>0</sub></sub>)*X**B**Î£*<sup>âˆ’1/2</sup>*Q*,
and *Î»*<sub>0*r*</sub>â‰¤ *Î»*<sub>0(*r*âˆ’1)</sub>â€„â‰¤â€„â‹¯â€„â‰¤â€„*Î»*<sub>01</sub>
are the *r* largest eigenvalues of
*Î£*<sup>âˆ’1/2</sup>*B*<sup>âŠ¤</sup>*X*<sup>âŠ¤</sup>(*I*<sub>*n*</sub>âˆ’*P*<sub>*H*<sub>0</sub></sub>)*X**B**Î£*<sup>âˆ’1/2</sup>.
Let
*Q*<sup>âŠ¤</sup>*Î£*<sup>âˆ’1/2</sup>*B*<sup>âŠ¤</sup>*X*<sup>âŠ¤</sup>(*I*<sub>*n*</sub>âˆ’*P*<sub>*H*<sub>0</sub></sub>)*X**B**Î£*<sup>âˆ’1/2</sup>*Q*â€„=â€„*U*<sup>âŠ¤</sup>diagâ€†(*Î»*<sub>1</sub>,â€¦,*Î»*<sub>*r*</sub>)*U*,
where the right-hand side is the eigenvalue decomposition of the left
and *U* is an *r*â€…Ã—â€…*r* orthogonal matrix. We have

$$\begin{aligned}
        Q^\top \Sigma^{-1 / 2} B^\top X^\top\left(I\_{n}-P\_{H\_{0}}\right) X B \Sigma^{-1 / 2} Q & =U^\top \operatorname{diag}\left(\lambda\_{1}, \ldots, \lambda\_{r}\right) U \\
        & \leq U^\top \operatorname{diag}\left(\lambda\_{01}, \ldots, \lambda\_{0 r}\right) U \\
        & =U^\top Q\_{0}^\top \Sigma^{-1 / 2} B^\top X^\top\left(I\_{n}-P\_{H\_{0}}\right) X B \Sigma^{-1 / 2} Q\_{0} U .
    \end{aligned}$$

So

$$\begin{aligned}
        M= & U^\top Q\_{0}^\top \Sigma^{-1 / 2} B^\top X^\top\left(I\_{n}-P\_{H\_{0}}\right) X B \Sigma^{-1 / 2} Q\_{0} U \\
        & -Q^\top \Sigma^{-1 / 2} B^\top X^\top\left(I\_{n}-P\_{H\_{0}}\right) X B \Sigma^{-1 / 2} Q
    \end{aligned}$$

is a semi-definite matrix. Generate
*K*â€„âˆ¼â€„*W*<sub>*k*</sub>(0,*I*<sub>*r*</sub>,*M*), which is a *r*â€…Ã—â€…*r*
random matrix of non-central Wishart distribution with zero degree of
freedom and non-central parameter *M* independent to (*X*,*Y*). Note
that *K* is semi-definite almost surely. From the property of
non-central Wishart distribution from Muirhead (2009) (Muirhead 2009),
we know that

$$H\_{\Sigma^{-1 / 2} Q}+K \stackrel{d}{=} H\_{\Sigma^{-1 / 2} Q\_{0} U} \stackrel{d}{=} H\_{\Sigma^{-1 / 2} Q\_{0}}$$

Since
$G\_{\Sigma^{-1 / 2} Q} \stackrel{d}{=} G\_{\Sigma^{-1 / 2} Q\_{0}}, G\_{\Sigma^{-1 / 2} Q}$
is independent to *H*<sub>*Î£*<sup>âˆ’1/2</sup>*Q*</sub>â€…+â€…*K* and
*G*<sub>*Î£*<sup>âˆ’1/2</sup>*Q*<sub>0</sub></sub> is independent to
*H*<sub>*Î£*<sup>âˆ’1/2</sup>*Q*<sub>0</sub></sub>, we have

$$\frac{\left\|G\_{\Sigma^{-1 / 2} Q}\right\|}{\left\|G\_{\Sigma^{-1 / 2} Q}+H\_{\Sigma^{-1 / 2} Q}+K\right\|} \stackrel{d}{=} \frac{\left\|G\_{\Sigma^{-1 / 2} Q\_{0}}\right\|}{\left\|G\_{\Sigma^{-1 / 2} Q\_{0}}+H\_{\Sigma^{-1 / 2} Q\_{0}}\right\|}$$

Thus, we have

$$\begin{aligned}
    P\left(\lambda\_{\Sigma^{-1 / 2} Q}\<c\right) & =P\left(\frac{\mid G\_{\Sigma^{-1 / 2} Q}}{\left\|G\_{\Sigma^{-1 / 2} Q}+H\_{\Sigma^{-1 / 2} Q}\right\|}\<c\right) \\
        & \leq P\left(\frac{\mid G\_{\Sigma^{-1 / 2} Q}}{\left\|G\_{\Sigma^{-1 / 2} Q}+H\_{\Sigma^{-1 / 2} Q}+K\right\|}\<c\right) \\
        & =P\left(\frac{\mid G\_{\Sigma^{-1 / 2} Q\_{0}}}{\left\|G\_{\Sigma^{-1 / 2} Q}+H\_{\Sigma^{-1 / 2} Q\_{0}}\right\|}\<c\right)=P\left(\lambda\_{\Sigma^{-1 / 2} Q\_{0}}\<c\right) .
\end{aligned}$$

Thus, *Î£*<sup>âˆ’1/2</sup>*Q*<sub>0</sub> is the optimal *p*â€…Ã—â€…*r*
projection matrix.

Proof of Theorem
<a href="#thm1" data-reference-type="ref" data-reference="thm1">1</a>(A2)
For any *p*â€…Ã—â€…*r* projection matrix *Q* and *r*â€„\>â€„*m* and from Theorem
<a href="#thm1" data-reference-type="ref" data-reference="thm1">1</a>(A2),
*P*<sub>0</sub>â€„=â€„*Î£*<sup>âˆ’1/2</sup>*Q*<sub>0</sub> is the optimal
*p*â€…Ã—â€…*r* projection direction matrix, where *Q*<sub>0</sub>=
(*R*<sub>1</sub>,â‹¯,*R*<sub>*r*</sub>) and *R*<sub>*i*</sub> is defined
as in Theorem
<a href="#thm1" data-reference-type="ref" data-reference="thm1">1</a> .
Note that
$P\_{0}=\left(\begin{array}{ll}P\_{01} & P\_{02}\end{array}\right)$,
where
*P*<sub>01</sub>â€„=â€„*Î£*<sup>âˆ’1/2</sup>(*R*<sub>1</sub>,â‹¯,*R*<sub>*m*</sub>)
is the optimal *p*â€…Ã—â€…*m* projection direction matrix and
*P*<sub>02</sub>=
*Î£*<sup>âˆ’1/2</sup>(*R*<sub>*m*â€…+â€…1</sub>,â‹¯,*R*<sub>*r*</sub>).

The LRT statistic for the *P*<sub>0</sub> projection test can be
decomposed correspondingly as follows:
$$\dfrac{\|G\_{P_0}\|}{\|G\_{P_0}+H\_{P_0}\|}=\dfrac{\|G\_{22}\|}{\|G\_{22}+H\_{22}\|}\dfrac{\left\|\begin{pmatrix}
    G\_{11} & G\_{12}\\
   G\_{21} & G\_{22}
\end{pmatrix}\right\|/\|G\_{22}\|}{\left\|\left(\begin{array}{cc}
    G\_{11}+H\_{11} & G\_{12}+H\_{12} \\
    G\_{21}+H\_{21} & G\_{22}+H\_{22}
    \end{array}\right)\right\| /\left\|G\_{22}+H\_{22}\right\|}$$
where *G*<sub>11</sub>,â€†*G*<sub>11</sub>â€…+â€…*H*<sub>11</sub> are of
dimension *m*â€…Ã—â€…*m*, and
*G*<sub>22</sub>,â€†*G*<sub>22</sub>â€…+â€…*H*<sub>22</sub> are of dimension
(*r*âˆ’*m*)Ã— (*r*âˆ’*m*).*G*<sub>22</sub> and
*G*<sub>22</sub>â€…+â€…*H*<sub>22</sub> can be seen as the residual sums of
squares under *H*<sub>1</sub> and *H*<sub>0</sub> for *P*<sub>02</sub>
projection test. It is easy to show that
(*G*<sub>22</sub>,*H*<sub>22</sub>) has the same distribution under
*H*<sub>1</sub> and *H*<sub>0</sub>. In fact, under *H*<sub>1</sub> and
*H*<sub>0</sub>,â€†*G*<sub>22</sub>â€„âˆ¼â€„*W*<sub>*r*â€…âˆ’â€…*m*</sub>(*n*âˆ’*d*,*I*<sub>*r*â€…âˆ’â€…*m*</sub>),â€†*H*<sub>22</sub>â€„âˆ¼â€„*W*<sub>*r*â€…âˆ’â€…*m*</sub>(*m*,*I*<sub>*m*</sub>)
and they are independent to each other. So
\|*G*<sub>22</sub>\|/\|*G*<sub>22</sub>+*H*<sub>22</sub>\| has the same
distribution under *H*<sub>1</sub> and *H*<sub>0</sub>. Moreover,
$\left\|\left(\begin{array}{ll}G\_{11} & G\_{12} \\ G\_{21} & G\_{22}\end{array}\right)\right\| /\left\|G\_{22}\right\|=\left\|G\_{11}-G\_{12} G\_{22}^{-1} G\_{21}\right\|$
can be seen as the generalized residual sum of squares of
*Y**P*<sub>01</sub> on *Y**P*<sub>02</sub> and *X*.
*W*<sub>1</sub>â€„=â€„*G*<sub>11</sub>â€…âˆ’â€…*G*<sub>12</sub>*G*<sub>22</sub><sup>âˆ’1</sup>*G*<sub>21</sub>â€„âˆ¼â€„*W*<sub>*m*</sub>(*n*âˆ’(*r*âˆ’*m*)âˆ’*d*,*I*<sub>*m*</sub>).

$$\left\|\left(\begin{array}{cc}
    G\_{11}+H\_{11} & G\_{12}+H\_{12} \\
    G\_{21}+H\_{21} & G\_{22}+H\_{22}
    \end{array}\right)\right\| /\left\|G\_{22}+H\_{22}\right\|=\mid G\_{11}+H\_{11}-\left(G\_{12}+H\_{12}\right)\left(G\_{22}+H\_{22}\right)^{-1}\left(G\_{21}+H\_{21}\right)$$

can be seen as the generalized residual sum of squares of
*Y**P*<sub>01</sub> on *Y**P*<sub>02</sub> and
*X**A*<sub>1</sub><sup>âŠ¤</sup>. *W*<sub>0</sub>=
*G*<sub>11</sub>â€…+â€…*H*<sub>11</sub>â€…âˆ’â€…(*G*<sub>12</sub>+*H*<sub>12</sub>)(*G*<sub>22</sub>+*H*<sub>22</sub>)<sup>âˆ’1</sup>(*G*<sub>21</sub>+*H*<sub>21</sub>)â€„=â€„*W*<sub>1</sub>â€…+â€…*W*<sub>2</sub>,
where
*W*<sub>2</sub>â€„âˆ¼â€„*W*<sub>*m*</sub>(*m*,*I*<sub>*m*</sub>,*P*<sub>01</sub><sup>âŠ¤</sup>*B*<sup>âŠ¤</sup>*X*<sup>âŠ¤</sup>(*I*<sub>*n*</sub>âˆ’
*P*<sub>*H*<sub>0</sub></sub>)*X**B**Î£*<sup>âˆ’1/2</sup>) and
*W*<sub>2</sub> is independent to *W*<sub>1</sub>.

In sum,

$$\frac{\left\|\left(\begin{array}{ll}
    G\_{11} & G\_{12} \\
    G\_{21} & G\_{22}
    \end{array}\right)\right\| /\left\|G\_{22}\right\|}{\left\|\left(\begin{array}{ll}
    G\_{11}+H\_{11} & G\_{12}+H\_{12} \\
    G\_{21}+H\_{21} & G\_{22}+H\_{22}
    \end{array}\right)\right\| /\left\|G\_{22}+H\_{22}\right\|} \stackrel{d}{=} \frac{\left\|W\_{1}\right\|}{\left\|W\_{1}+W\_{2}\right\|},$$

where
*W*<sub>1</sub>â€„âˆ¼â€„*W*<sub>*m*</sub>(*n*âˆ’(*r*âˆ’*m*)âˆ’*d*,*I*<sub>*m*</sub>),â€†*W*<sub>2</sub>â€„âˆ¼â€„*W*<sub>*m*</sub>(*m*,*I*<sub>*m*</sub>,*P*<sub>01</sub><sup>âŠ¤</sup>*B*<sup>âŠ¤</sup>*X*<sup>âŠ¤</sup>(*I*<sub>*n*</sub>âˆ’*P*<sub>*H*<sub>0</sub></sub>)*X**B**P*<sub>01</sub>),
and *W*<sub>1</sub> is dependent to *W*<sub>2</sub>.

Note that the distribution of
$\frac{\left\|W\_{1}\right\|}{\left\|W\_{1}+W\_{2}\right\|}$ does not
depend on *Y**P*<sub>02</sub>. So it is independent to
$\frac{\left\|G\_{22}\right\|}{\left\|G\_{22}+H\_{22}\right\|}$. The
proof here is similar to the proof in Lemma 8.4.3, Lemma 8.4.4 in
Anderson (2003) (Anderson 2003), which decomposes the test statistic
into the product of a series of independent Beta distributed variables.

Since $\frac{\left\|G\_{22}\right\|}{\left\|G\_{22}+H\_{22}\right\|}$
has the same distribution under *H*<sub>0</sub> and *H*<sub>1</sub>, the
test statistic

$$\frac{\left\|\left(\begin{array}{cc}
    G\_{11} & G\_{12} \\
    G\_{21} & G\_{22}
    \end{array}\right)\right\| /\left\|G\_{22}\right\|}{\left\|\left(\begin{array}{cc}
    G\_{11}+H\_{11} & G\_{12}+H\_{12} \\
    G\_{21}+H\_{21} & G\_{22}+H\_{22}
    \end{array}\right)\right\| /\left\|G\_{22}+H\_{22}\right\|}$$

is better than

$$\frac{\left\|G\_{22}\right\|}{\left\|G\_{22}+H\_{22}\right\|} \frac{\left\|\left(\begin{array}{ll}
    G\_{11} & G\_{12} \\
    G\_{21} & G\_{22}
    \end{array}\right)\right\| /\left\|G\_{22}\right\|}{\left\|\left(\begin{array}{ll}
    G\_{11}+H\_{11} & G\_{12}+H\_{12} \\
    G\_{21}+H\_{21} & G\_{22}+H\_{22}
    \end{array}\right)\right\| /\left\|G\_{22}+H\_{22}\right\|} .$$

Moreover,
$\frac{\left\|\left(\begin{array}{ll}G\_{11} & G\_{12} \\ G\_{21} & G\_{22}\end{array}\right)\right\| /\left\|G\_{22}\right\|}{\left\|\left(\begin{array}{ll}G\_{11}+H\_{11} & G\_{12}+H\_{12} \\ G\_{21}+H\_{21} & G\_{22}+H\_{22}\end{array}\right)\right\| /\left\|G\_{22}+H\_{22}\right\|}$
has the same distribution with *Î›*<sub>*P*<sub>01</sub></sub> only with
sample size reduced from *n* to *n*â€…âˆ’â€…(*r*âˆ’*m*).

Thus, *P*<sub>01</sub> projection test is powerful than
$\left(\begin{array}{ll}P\_{01} & P\_{02}\end{array}\right)$ projection
test. So the optimal *m* dimension projection test is better than the
optimal *r* dimension projection test with *r*â€„\>â€„*m*, which means that
the dimension for the optimal projection direction is less than or equal
to *m* under normality assumption.

Proof of Theorem
<a href="#thm1" data-reference-type="ref" data-reference="thm1">1</a>(A3).
Theorem
<a href="#thm1" data-reference-type="ref" data-reference="thm1">1</a>(A3)
is just a special case of Theorem
<a href="#thm1" data-reference-type="ref" data-reference="thm1">1</a>(A1)
and can be proved using the same techniques.

Proof of Theorem
<a href="#thm1" data-reference-type="ref" data-reference="thm1">1</a>
(B). It is sufficient to prove that the test statistic
$\Lambda=\frac{\left\|G\_{P}\right\|}{\left\|G\_{P}+H\_{P}\right\|}$
follows the same asymptotic distribution in both Gaussian and
non-Gaussian multivariate linear models, thus the asymptotic results in
the Gaussian case also hold in the non-Gaussian cases.

From Conditions 1, 2, and 3 of Theorem
<a href="#thm1" data-reference-type="ref" data-reference="thm1">1</a>,
we have $\frac{G\_{P}}{n} \stackrel{p}{\rightarrow} P^\top \Sigma P$
with convergence rate *O*<sub>*p*</sub>(*n*<sup>âˆ’1/2</sup>). So with the
delta method, we have

$$\begin{aligned}
        -\log (\Lambda) & =-\log \frac{\left\|G\_{P}\right\|}{\left\|G\_{P}+H\_{P}\right\|}=\log \left(\left\|I\_{k}+H\_{P} G\_{P}^{-1}\right\|\right) \\
        & =\log \operatorname{det}\left(I\_{k}+H\_{P}\left(P^\top \Sigma P\right)^{-1} / n\right)\left(1+O\_{p}\left(n^{-1 / 2}\right)\right)
    \end{aligned}$$

Hence the theorem holds if *H*<sub>*P*</sub> follows the same asymptotic
distribution under both the Gaussian and non-Gaussian cases.

For any projection direction matrix *P* of dimension *p*â€…Ã—â€…*k*, we have
*BÌ‚*<sub>*I*<sub>1</sub></sub><sup>\*</sup>â€„=â€„*BÌ‚*<sup>\*</sup>=
(*X*<sup>âŠ¤</sup>*X*)<sup>âˆ’1</sup>*X*<sup>âŠ¤</sup>*Y**P*, and
*BÌ‚*<sub>*H*<sub>0</sub></sub><sup>\*</sup>â€„=â€„*A*<sub>1</sub>(*A*<sub>1</sub><sup>âŠ¤</sup>*X*<sup>âŠ¤</sup>*X**A*<sub>1</sub>)<sup>âˆ’1</sup>*A*<sub>1</sub><sup>âŠ¤</sup>*X*<sup>âŠ¤</sup>*Y**P*,
where *A*<sub>1</sub>â€„=â€„(*A*<sub>0</sub><sup>âŠ¤</sup>)<sup>âŠ¥</sup>.
Hence,

$$d\_{B}=\hat{B\_{H\_{1}}^\*}-\hat{B\_{H\_{0}}^\*}=\left(\left(X^\top X\right)^{-1}-A\_{1}\left(A\_{1}^\top X^\top X A\_{1}\right)^{-1} A\_{1}^\top\right) X^\top Y P$$

Then from Conditions 1, 2 and 3 of Theorem
<a href="#thm1" data-reference-type="ref" data-reference="thm1">1</a>,
we have $d\_{B}=\hat{B\_{H\_{1}}^\*}-\hat{B\_{H\_{0}}^\*}$ has a
limiting normal distribution with

$$\begin{aligned}
        \mathbb{E} d\_{B} & =\left(\left(X^\top X\right)^{-1}-A\_{1}\left(A\_{1} X^\top X A\_{1}\right)^{-1} A\_{1}^\top\right) X^\top X B P \\
        & =\left(I\_{d}-A\_{1}\left(A\_{1}^\top X^\top X A\_{1}\right)^{-1} A\_{1}^\top X^\top X\right) B P
    \end{aligned}$$

and
$\sqrt{n} \operatorname{vec}\left(d\_{B}-\mathbb{E} d\_{B}\right) \stackrel{d}{\rightarrow} N\_{d k}(0, V)$,
where

$$\begin{aligned}
        V & =\lim \_{n \rightarrow \infty}\left(\left(X^\top X / n\right)^{-1}-A\_{1}\left(A\_{1}^\top X^\top X A\_{1} / n\right)^{-1} A\_{1}^\top\right) \otimes\left(P^\top \Sigma P\right) \\
        & =\left(M\_{X}^{-1}-A\_{1}\left(A\_{1}^\top M\_{X} A\_{1}\right)^{-1} A\_{1}^\top\right) \otimes\left(P^\top \Sigma P\right) .
    \end{aligned}$$

Hence we have
$H\_{P}=d\_{B}^\top X^\top X d\_{B} \stackrel{d}{\rightarrow} W\_{k}\left(m, I\_{k}, C\right)$,
where *C* is the non-central parameter for the Wishart distribution and
*C*â€„=â€„*P*<sup>âŠ¤</sup>*B*<sup>âŠ¤</sup>*X*<sup>âŠ¤</sup>(*I*<sub>*n*</sub>âˆ’*X**A*<sub>1</sub>(*A*<sub>1</sub><sup>âŠ¤</sup>*X*<sup>âŠ¤</sup>*X**A*<sub>1</sub>)<sup>âˆ’1</sup>*A*<sub>1</sub><sup>âŠ¤</sup>*X*<sup>âŠ¤</sup>)*X**B**P*.

Hence we prove that *H*<sub>*P*</sub> follows the same asymptotic
distribution in both the Gaussian and non-Gaussian cases under
Conditions 1, 2, and 3.Â â—»

# Estimation of Optimal Projection Direction

Theorem
<a href="#thm1" data-reference-type="ref" data-reference="thm1">1</a>
explains how the optimal projection direction is determined by *Î£* and
*B*. We shall devise an estimation technique for the optimal direction.
We recommend setting *k*â€„=â€„*m* for the mentioned hypothesis above with
small *m*, as we did for the one-sample and two-sample mean testing
issues, and the optimal projection direction is
*Î£*<sup>âˆ’1</sup>*B*<sup>âŠ¤</sup>*A*<sub>0</sub><sup>âŠ¤</sup> according to
Theorems 1(*A*2) and (*A*3). As seen below, the ideal projection matrix
can be computed column by column.
*P*<sub>0,â€†*k*</sub>â€„=â€„*Î£*<sup>âˆ’1</sup>*B*<sup>âŠ¤</sup>*A*<sub>0,â€†*k*</sub><sup>âŠ¤</sup>â€Šâ€âˆ€*k*â€„=â€„1,â€†2,â€†â‹¯,â€†*m*
where *A*<sub>0,â€†*k*</sub> is the *k*<sup>*t**h*</sup> row of
*A*<sub>0</sub> and *P*<sub>0,â€†*k*</sub> is the *k*<sup>*t**h*</sup>
column of *P*<sub>0</sub>. So without loss of generality we can assume,
*m*â€„=â€„1. Now we have to estimate the optimal projection direction
*P*<sub>0</sub>â€„=â€„*Î£*<sup>âˆ’1</sup>*B*<sup>âŠ¤</sup>*A*<sub>0</sub><sup>âŠ¤</sup>
So basically the estimate can be obtained as,
*PÌ‚*â€„=â€„*Î£Ì‚*<sup>âˆ’1</sup>*BÌ‚*<sup>âŠ¤</sup>*A*<sub>0</sub><sup>âŠ¤</sup>
However, when *p* is larger than *m*, then *Î£Ì‚* may become singular. To
deal with this problem, we will use an shrinkage estimator like ridge
regression to find,
*PÌ‚*<sub>ridge</sub>â€„=â€„(*Î»*<sub>0</sub>*I*<sub>*p*</sub>+*Î£Ì‚*)<sup>âˆ’1</sup>*BÌ‚*<sup>âŠ¤</sup>*A*<sub>0</sub><sup>âŠ¤</sup>
where *Î»*<sub>0</sub>â€„\>â€„0. Now to find the estimate of and *BÌ‚*, we use
the ordinary least squares i.e.,
*BÌ‚*<sub>LS</sub>â€„=â€„(*X*<sup>âŠ¤</sup>*X*)<sup>âˆ’1</sup>*X*<sup>âŠ¤</sup>*Y*
and estimate *Î£Ì‚* by the sample covariance of the residuals i.e.,
$$\hat{\Sigma}\_{\text{LS}} = \dfrac{1}{n-d}(Y-X\hat{B}\_{\text{LS}})^\top(Y-X\hat{B}\_{\text{LS}})$$
Although having prior knowledge of *B* and *Î£*, using specific
estimators for them can increase the power of the test, we advise using
general estimators of *B* and *Î£* in this article to reflect the general
case. Now from *BÌ‚*<sub>LS</sub> and *Î£Ì‚*<sub>LS</sub> we have,
*PÌ‚*<sub>LS</sub>â€„=â€„(*Î»*<sub>0</sub>*I*<sub>*p*</sub>+*Î£Ì‚*<sub>LS</sub>)<sup>âˆ’1</sup>*BÌ‚*<sub>LS</sub><sup>âŠ¤</sup>*A*<sub>0</sub><sup>âŠ¤</sup>

# U-Projection test

Consider the general multivariate linear model
(<a href="#model" data-reference-type="ref" data-reference="model"><span
class="math display"><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></span></a>)
and the hypothesis *A*<sub>0</sub>*B*â€„=â€„0. From Theorem
<a href="#thm1" data-reference-type="ref" data-reference="thm1">1</a>,
*P*<sub>0</sub>â€„=â€„*Î£*<sup>âˆ’1</sup>*B*<sup>âŠ¤</sup>*A*<sub>0</sub><sup>âŠ¤</sup>
is the asymptotic optimal projection matrix of dimension *p*â€…Ã—â€…*m*. The
null hypothesis of the projection test with direction *P*<sub>0</sub> is
*H*<sub>0*P*</sub>â€„:â€„*A*<sub>0</sub>*B**P*<sub>0</sub>â€„=â€„*A*<sub>0</sub>*B**Î£*<sup>âˆ’1</sup>*B*<sup>âŠ¤</sup>*A*<sub>0</sub><sup>âŠ¤</sup>â€„=â€„0.
Since
*A*<sub>0</sub>*B**Î£*<sup>âˆ’1</sup>*B*<sup>âŠ¤</sup>*A*<sub>0</sub><sup>âŠ¤</sup>
is positive semi-definite, it is equivalent to the test
*H*<sub>0*P*</sub>â€„:â€„*t**r*(*A*<sub>0</sub>*B**P*<sub>0</sub>)â€„=â€„*t**r*(*A*<sub>0</sub>*B**Î£*<sup>âˆ’1</sup>*B*<sup>âŠ¤</sup>*A*<sub>0</sub><sup>âŠ¤</sup>)â€„=â€„0.
Following the sample-splitting test procedure, we split the data
(*X*,*Y*) into two parts (*X*<sub>1</sub>,*Y*<sub>1</sub>) and
(*X*<sub>2</sub>,*Y*<sub>2</sub>). The sample sizes of
*X*<sub>1</sub>,â€†*Y*<sub>1</sub> are *k*, and the sample sizes of
*X*<sub>2</sub>,â€†*Y*<sub>2</sub> are *n*â€…âˆ’â€…*k*. The first sample is used
an estimator *PÌ‚*<sub>(*X*<sub>1</sub>,*Y*<sub>1</sub>)</sub> of the
*m*-dimensional projection and sample-splitting test statistic on the
projected second sample is calculated as follows:
*t**r*(*A*<sub>0</sub>*BÌ‚*<sub>(*X*<sub>2</sub>,*Y*<sub>2</sub>)</sub>*PÌ‚*<sub>(*X*<sub>1</sub>,*Y*<sub>1</sub>)</sub>)â€„=â€„*t**r*(*A*<sub>0</sub>(*X*<sub>2</sub><sup>âŠ¤</sup>*X*<sub>2</sub>)<sup>âˆ’1</sup>*X*<sub>2</sub><sup>âŠ¤</sup>*Y*<sub>2</sub>*PÌ‚*<sub>(*X*<sub>1</sub>,*Y*<sub>1</sub>)</sub>)
So the subsequent U-projection statistic can be written as,
$$U_p = \dfrac{1}{\|\Gamma\|} \sum\_{\gamma\in\Gamma} tr\left( A_0(X\_{-\gamma}^\top X\_{-\gamma})^{-1}X\_{-\gamma}^\top Y\_{-\gamma}\hat{P}\_{(X\_{\gamma},Y\_{\gamma})} \right) \tag{3}$$
where *Î“*â€„=â€„{*Î³*\|*Î³*âŠ‚{1,2,â‹¯,*n*},\|*Î³*\|â€„=â€„*k*,
rank(*X*<sub>*Î³*</sub>)â€„\>â€„*d*, rank(*X*<sub>âˆ’*Î³*</sub>)â€„â‰¥â€„*d*}, and
*X*<sub>*Î³*</sub>,â€†*Y*<sub>*Î³*</sub>,â€†*X*<sub>âˆ’*Î³*</sub>,â€†*Y*<sub>âˆ’*Î³*</sub>
are subsamples of *X* and *Y* with and without index *Î³*, respectively.
Now we can have the asymptotic distribution of *U*<sub>*p*</sub> using
the following theorem:

**Theorem 2**. *Consider *H*<sub>0</sub>â€„:â€„*A*<sub>0</sub>*B*â€„=â€„0 under
model (1). Let
$$h\_{P}\left(Z\_{1,1}, \ldots, Z\_{k+d, 1} ; Z\_{1,2}, \ldots, Z\_{k+d, 2}\right)
= \frac{1}{\|\Gamma\|} \sum\_{\gamma \in \Gamma} \operatorname{diag}\left(A\_{0}\left(Z\_{-\gamma, 1}^\top Z\_{-\gamma, 1}\right)^{-1} Z\_{-\gamma, 1}^\top Z\_{-\gamma, 2} \hat{P}\_{\left(Z\_{\gamma, 1}, Z\_{\gamma, 2}\right)}\right)$$
where â€…âˆ’â€…*Î³*â€„=â€„{1,â€†â€¦,â€†*k*â€…+â€…*d*}â€…âˆ–â€…*Î³*,â€†*Î“*â€„=â€„{*Î³*,â€†*Î³*â€„âŠ‚â€„{1,â€†2,â€†â€¦,â€†*k*+
*d*},\|*Î³*\|=*k*,rank(*Z*<sub>*Î³*,â€†1</sub>)\>*d*,rank(*Z*<sub>âˆ’*Î³*,â€†1</sub>)=*d*}
; *Z*<sub>*Î³*,â€†*i*</sub>,â€†*Z*<sub>âˆ’*Î³*,â€†*i*</sub> are samples of
*Z*<sub>â€¦,â€†*i*</sub> with and without index *Î³* for *i*â€„=â€„1,â€†2; and
*PÌ‚*<sub>(*Z*<sub>*Î³*,â€†1</sub>,*Z*<sub>*Î³*,â€†2</sub>)</sub> is the
projection direction estimated with
(*Z*<sub>*Î³*,â€†1</sub>,*Z*<sub>*Î³*,â€†2</sub>). Then with the conditions
that *k* is fixed and that
ğ”¼\[tr(cov(*h*<sub>*P*</sub>(*Z*<sub>1,â€†1</sub>,â€¦,*Z*<sub>*k*â€…+â€…*d*,â€†1</sub>;*Z*<sub>1,â€†2</sub>,â€¦,*Z*<sub>*k*â€…+â€…*d*,â€†2</sub>)))\]â€„\<â€„*C*<sub>0</sub>
for some fixed *C*<sub>0</sub>â€„\>â€„0 and
(*Z*<sub>*i*,â€†1</sub>,*Z*<sub>*i*,â€†2</sub>),â€†*i*â€„=â€„1,â€†â€¦,â€†*k*â€…+â€…*d*
i.i.d. from the distribution of (*X*,*Y*), we have asymptotic normality
of the general U-projection statistic (3):
$$\sqrt{n}\left(U\_{P}-\mathbb{E} \[U\_{P}\]\right) \stackrel{d}{\rightarrow} N\left(0,(k+d)^{2} 1\_{m}^\top \Xi\_{1} 1\_{m}\right)$$
where 1<sub>*m*</sub> is a vector full of one with length *m*, and
*Î*<sub>1</sub>â€„=â€„covâ€†(*h*<sub>*P*</sub>(*Z*<sub>1,â€†1</sub>,*Z*<sub>2,â€†1</sub>,â€¦,*Z*<sub>*k*â€…+â€…*d*,â€†1</sub>;*Z*<sub>1,â€†2</sub>,â€¦,*Z*<sub>*k*â€…+â€…*d*,â€†2</sub>),*h*<sub>*P*</sub>(*Z*<sub>1,â€†1</sub>,*Z*<sub>2,â€†1</sub><sup>â€²</sup>,â€¦,*Z*<sub>*k*â€…+â€…*d*,â€†1</sub><sup>â€²</sup>;*Z*<sub>1,â€†2</sub>,*Z*<sub>2,â€†2</sub><sup>â€²</sup>,â€¦,*Z*<sub>*k*â€…+â€…*d*,â€†2</sub><sup>â€²</sup>))
for (*Z*<sub>*i*,â€†1</sub>,*Z*<sub>*i*,â€†2</sub>),â€†*i*â€„=â€„1,â€†â€¦,â€†*k*â€…+â€…*d*
and
(*Z*<sub>*i*,â€†1</sub><sup>â€²</sup>,*Z*<sub>*i*,â€†2</sub><sup>â€²</sup>),â€†*i*â€„=â€„2,â€†â€¦,â€†*k*â€…+â€…*d*
i.i.d. from the distribution of (*X*,*Y*). Furthermore, under the null
hypothesis, we have ğ”¼\[*U*<sub>*P*</sub>\]â€„=â€„0. *

*Proof.* Let
$$M\_{P}=\frac{1}{\|\Gamma\|} \sum\_{\gamma \in \Gamma} \operatorname{diag}\left(A\_{0}\left(X\_{-\gamma}^{T} X\_{-\gamma}\right)^{-1} X\_{-\gamma}^{T} Y\_{-\gamma} \hat{P}\_{\left(X\_{\gamma}, Y\_{\gamma}\right)}\right)$$
where
*Î“*â€„=â€„{*Î³*\|*Î³*âŠ‚{1,2,â‹¯,*n*},\|*Î³*âˆ£=*k*,rank(*X*<sub>*Î³*</sub>)\>*d*,rank(*X*<sub>âˆ’*Î³*</sub>)â‰¥*d*},
and *X*<sub>*Î³*</sub>,â€†*Y*<sub>*Î³*</sub>,
*X*<sub>âˆ’*Î³*</sub>*Y*<sub>âˆ’*Î³*</sub> are sub-samples of *X* and *Y* with
and without index *Î³* respectively. Then we have
*U*<sub>*P*</sub>â€„=â€„1<sub>*m*</sub><sup>*T*</sup>*M*<sub>*P*</sub>.
*M*<sub>*P*</sub> can be rewritten as a U-statistic from the kernel
*h*<sub>*P*</sub>. From the asymptotic normality result of U-statistics,
we have
$$\sqrt{n}\left(M\_{P}-\mathbb{E} \[M\_{P}\]\right) \stackrel{d}{\rightarrow} N\left(0,(k+d)^{2} \Xi\_{1}\right)$$
where *Î*<sub>1</sub> is as defined in the theorem. Hence the asymptotic
result of *U*<sub>*P*</sub> follows from the fact that
*U*<sub>*P*</sub>â€„=â€„1<sub>*m*</sub><sup>*T*</sup>*M*<sub>*P*</sub>.
Moreover, it is easy to get
ğ”¼<sub>*H*<sub>0</sub></sub>\[*U*<sub>*P*</sub>\]â€„=â€„0 under the null
hypothesis. This completes the proof of the theorem.â—»

Although the asymptotic distribution of the U-projection statistic can
be obtained under certain assumptions, as shown in Theorem
<a href="#thm2" data-reference-type="ref" data-reference="thm2">2</a>,
it may not be very useful here for a number of reasons:

-   If the sample size *n* is small, the asymptotic distribution will be
    far from the real distribution.

-   In general, the relationship between the asymptotic variance under
    the null and the covariance is rather complex.

-   The (asymptotic) distribution of the statistic is likewise affected
    by the projection direction estimation method. The distribution will
    change if we use alternative estimation methods for the projection
    direction.

So using randomization, we can calculate the p-value for the test using
the following algorithm:

**P-value Calculation using Randomization**

**Input** *n*â€…Ã—â€…*d* dimensional matrix *X*, *n*â€…Ã—â€…*p* dimensional matrix
*Y*, *m*â€…Ã—â€…*d* dimensional matrix *A*<sub>0</sub>,
*d*â€„\<â€„*k*â€„â‰¤â€„*n*â€…âˆ’â€…*d*, and randomization times *N*.

Calculate the *U*-projection statistic *U*<sub>0</sub> on the original
dataset *X* and *Y*.<br> Let *i*â€„=â€„0<br> **For** (j=1:N){<br> â€ƒ Do
randomization, get *X*â€² and *Y*â€²<br> â€ƒ Calculate the *U*-projection
statistic *U*â€² on the randomized dataset *X*â€² and *Y*â€².<br> â€ƒ **If**
(*U*â€²â‰¥*U*){ <br> â€ƒ â€ƒ *i*â€„=â€„*i*â€…+â€…1<br> â€ƒ }<br> }<br> Calculate p-value
by $p=\dfrac{i}{N}$

## Examples

### One Sample Mean Testing:

For one-sample mean testing, suppose we have *n* samples *Y* from a
*p*-dimensional multivariate distribution with mean *Î¼* and covariance
*Î£*, and we want to test whether *Î¼*â€„=â€„0. Now for this setting we can
rewrite our model as,
*Y*â€„=â€„1<sub>*n*</sub>*B*â€…+â€…*E*
where 1<sub>*n*</sub> is a column vector full of 1 of length *n*,
*B*â€„=â€„*Î¼*<sup>âŠ¤</sup>, *E* is the *n*â€…Ã—â€…*p* random error matrix, and the
null hypothesis becomes
*H*<sub>0</sub>â€„:â€„*B*â€„=â€„0
i.e., *A*<sub>0</sub>â€„=â€„1 and is of rank 1 in this problem. By Theorem
<a href="#thm1" data-reference-type="ref" data-reference="thm1">1</a>,
the optimal projection dimension is 1 and the optimal projection
direction is *Î£*<sup>âˆ’1</sup>*Î¼*. Now we can find the estimates of *Î£*
and *Î¼* through least squares and subsequently estimate the optimal
projection direction. So the estimated projection matrix becomes,
*PÌ‚*â€„=â€„(*Î»*<sub>0</sub>*I*<sub>*p*</sub>+*Î£Ì‚*)<sup>âˆ’1</sup>*BÌ‚*<sub>LS</sub><sup>âŠ¤</sup>1<sub>*n*</sub><sup>âŠ¤</sup>â€„=â€„(*Î»*<sub>0</sub>*I*<sub>*p*</sub>+*Î£Ì‚*)<sup>âˆ’1</sup>*YÌ„*
Thus, the U-projection statistic becomes
$$U\_{\mathrm{P}}=  \dfrac{1}{\binom{n}{k}}\sum\_{\gamma \in \Gamma}\text{tr}\left(\bar{Y}\_{-\gamma}^\top \times\left\\\lambda\_{0} I\_{p}+S\_{Y\_{\gamma}}\right\\^{-1} \times \bar{Y}\_{\gamma}\right)$$
where for *k* is the number of independent samples from *Y* to estimate
the projection direction, *Î“* is collections of all subsets of {1,â€¦,*n*}
with size *k*, and *Y*<sub>*Î³*</sub> and *Y*<sub>âˆ’*Î³*</sub> are subsets
of *Y* with and without index *Î³* correspondingly.

### Testing of Significance of Predictors

Consider the general linear model (1). Testing the significance of a
particular predictor, *X*<sup>(*i*)</sup>, is of interest. In other
words, take
*H*<sub>0</sub>â€„:â€„*A*<sub>0</sub>*B*â€„=â€„0
where *A*<sub>0</sub>â€„=â€„*e*<sub>*i*</sub><sup>âŠ¤</sup> and
*e*<sub>*i*</sub> is a column vector in *d* dimensions, with the
*i*<sup>*t**h*</sup> element being one and the remaining elements being
zero. The above hypothesis testing problem is also testing for the
conditional independence of *Y* and *X*<sup>(*i*)</sup> given other
(*d*âˆ’1) predictors under the normality assumption. This is because, in
the normal case, testing whether the regression coefficients equal zero
or not is equivalent to testing of conditional independence. According
to Theorem
<a href="#thm1" data-reference-type="ref" data-reference="thm1">1</a>,
the ideal projection dimension is 1 and the optimal projection direction
is *Î£*<sup>âˆ’1</sup>*B*<sup>âŠ¤</sup>*e*<sub>*i*</sub>. Now using the
previously demonstrated estimation method, we can estimate the
projection matrix by,
*PÌ‚*â€„=â€„(*Î»*<sub>0</sub>*I*<sub>*p*</sub>+*Î£Ì‚*)<sup>âˆ’1</sup>*BÌ‚*<sub>LS</sub><sup>âŠ¤</sup>*e*<sub>*i*</sub>â€„=â€„(*Î»*<sub>0</sub>*I*<sub>*p*</sub>+*Î£Ì‚*)<sup>âˆ’1</sup>*BÌ‚*<sub>*i*</sub>
Thus, the U-projection statistic becomes
$$U\_{\mathrm{P}}=  \dfrac{1}{\binom{n}{k}}\sum\_{\gamma \in \Gamma}\text{tr}\left(\hat{B}\_i^\top(Y\_{-\gamma}) \times\left\\\lambda\_{0} I\_{p}+S\_{Y\_{\gamma}}\right\\^{-1} \times \hat{B}\_i(Y\_{\gamma}) \right)$$
where for *k* is the number of independent samples from *Y* to estimate
the projection direction, *Î“* is collections of all subsets of {1,â€¦,*n*}
with size *k*, and *Y*<sub>*Î³*</sub> and *Y*<sub>âˆ’*Î³*</sub> are subsets
of *Y* with and without index *Î³* correspondingly.

# Future Direction: Multiple Sample Mean Testing

For general unbalanced multiple-sample mean testing problem, suppose for
*k*â€„=â€„1,â€†2,â€†â‹¯,â€†*K*, we have *n*<sub>*k*</sub> samples *Y*<sub>*k*</sub>
from a *p*-dimensional multivariate distribution *F*<sub>*k*</sub> with
mean *Î¼*<sub>*k*</sub> and covariance *Î£*, and we want to test whether
*Î¼*<sub>1</sub>â€„=â€„*Î¼*<sub>2</sub>â€„=â€„â‹¯â€„=â€„*Î¼*<sub>*k*</sub>. The problem
can be reformulated as
$$\begin{pmatrix}
    Y_1\\
    \cdots\\
    Y_k
\end{pmatrix} = \begin{pmatrix}
    1\_{n_1} & 0 & \cdots\\
    \cdots & \cdots & \cdots\\
    0 & \cdots & 1\_{n_k}
\end{pmatrix} B + E$$
where
*B*â€„=â€„(*Î¼*<sub>1</sub>,*Î¼*<sub>2</sub>,â‹¯,*Î¼*<sub>*k*</sub>)<sup>âŠ¤</sup>,
and *E* is the $\left( \sum\_{i=1}^k n_i\right)\times p$ random error
matrix. We want to test
*H*<sub>0</sub>â€„:â€„*A*<sub>0</sub>*B*â€„=â€„0
where
*A*<sub>0</sub>â€„=â€„{*a*<sub>*i**j*</sub>}<sub>1â€„â‰¤â€„*i*â€„â‰¤â€„*K*â€…âˆ’â€…1,â€†1â€„â‰¤â€„*j*â€„â‰¤â€„*K*</sub>
with *a*<sub>*i**i*</sub>â€„=â€„*K*â€…âˆ’â€…1 and
*a*<sub>*i**j*</sub>â€„=â€„â€…âˆ’â€…1,â€†âˆ€*i*â€„â‰ â€„*j*. In this problem,
*A*<sub>0</sub> is of rank *K*â€…âˆ’â€…1. By Theorem
<a href="#thm1" data-reference-type="ref" data-reference="thm1">1</a>,
the optimal projection dimension is less than or equal to *K*â€…âˆ’â€…1 and
the optimal *p*â€…Ã—â€…(*K*âˆ’1) projection direction matrix is
*K**Î£*<sup>âˆ’1</sup>(*Î¼*<sub>1</sub>âˆ’*Î¼Ì„*,â‹¯,*Î¼*<sub>*k*â€…âˆ’â€…1</sub>âˆ’*Î¼Ì„*),
where $\bar{\mu} = \frac{1}{k}\sum\_{k=1}^K \mu_k$. In particular,when
*K*â€„=â€„2, the optimal projection dimension is 1 and the optimal
projection direction is
*Î£*<sup>âˆ’1</sup>(*Î¼*<sub>1</sub>âˆ’*Î¼*<sub>2</sub>). Now using the
previously demonstrated estimation method, we can estimate the
projection matrix by,
*PÌ‚*<sub>LS</sub>â€„=â€„(*Î»*<sub>0</sub>*I*<sub>*p*</sub>+*Î£Ì‚*)<sup>âˆ’1</sup>*BÌ‚*<sub>LS</sub><sup>âŠ¤</sup>*A*<sub>0</sub><sup>âŠ¤</sup>â€„=â€„(*Î»*<sub>0</sub>*I*<sub>*p*</sub>+*Î£Ì‚*)<sup>âˆ’1</sup>(*YÌ„*<sub>1</sub>âˆ’*YÌ„*<sub>2</sub>)
Thus, the U-projection statistic becomes
$$U\_{\mathrm{LS}}=  \dfrac{1}{\binom{n_1}{k_1}\binom{n_2}{k_2}} \sum\_{\gamma\_{1} \in \Gamma\_{1}} \sum\_{\gamma\_{2} \in \Gamma\_{2}}\left(\bar{Y}\_{-\gamma\_{1}, 1}^\top-\bar{Y}\_{-\gamma\_{2}, 2}^\top\right) \times\left\\\lambda\_{0} I\_{p}+\frac{\left(k\_{1}-1\right) S\_{Y\_{\gamma\_{1}, 1}}+\left(k\_{2}-1\right) S\_{Y\_{\gamma\_{2}, 2}}}{k\_{1}+k\_{2}-2}\right\\^{-1} \times\left(\bar{Y}\_{\gamma\_{1}, 1}-\bar{Y}\_{\gamma\_{2}, 2}\right)$$
where for *i*â€„=â€„1,â€†2,â€†*k*<sub>*i*</sub> is the number of independent
samples from *Y*<sub>*i*</sub> to estimate the projection direction,
*Î“*<sub>*i*</sub> is collections of all subsets of
{1,â€¦,*n*<sub>*i*</sub>} with size *k*<sub>*i*</sub>, and
*Y*<sub>*Î³*,â€†*i*</sub> and *Y*<sub>âˆ’*Î³*,â€†*i*</sub> are subsets of
*Y*<sub>*i*</sub> with and without index *Î³* correspondingly. Now
consider the following condition and theorems:

**Condition 4:**
*E*<sub>*i*</sub>â€„=â€„*Î“**Z*<sub>*i*</sub>â€Šâ€ for *i*â€„=â€„1,â€†â€¦,â€†*n*, where
*Î“* is a *p*â€…Ã—â€…*t* matrix with some *t*â€„â‰¥â€„*p* such that
*Î“**Î“*<sup>*T*</sup>= *Î£*, and
*Z*<sub>*i*</sub>â€„=â€„(*Z*<sub>*i*,â€†1</sub>,â€¦,*Z*<sub>*i*,â€†*t*</sub>) are
*t*-variate independent and identically distributed random vectors
satisfying ğ”¼(*Z*<sub>*i*</sub>)â€„=â€„0,
varâ€†(*Z*<sub>*i*</sub>)â€„=â€„*I*<sub>*t*</sub>,â€†ğ”¼(*Z*<sub>*i*,â€†*k*</sub><sup>3</sup>)â€„=â€„0,â€†ğ”¼(*Z*<sub>*i*,â€†*k*</sub><sup>6</sup>)
is uniformly bounded, and
ğ”¼(*Z*<sub>*i*,â€†*l*<sub>1</sub></sub><sup>*Î±*<sub>1</sub></sup>*Z*<sub>*i*,â€†*l*<sub>2</sub></sub><sup>*Î±*<sub>2</sub></sup>â‹¯*Z*<sub>*i*,â€†*l*<sub>*s*</sub></sub><sup>*Î±*<sub>*s*</sub></sup>)â€„=â€„ğ”¼(*Z*<sub>*i*,â€†*l*<sub>1</sub></sub><sup>*Î±*<sub>1</sub></sup>)ğ”¼(*Z*<sub>*i*,â€†*l*<sub>2</sub></sub><sup>*Î±*<sub>2</sub></sup>)â‹¯ğ”¼(*Z*<sub>*i*,â€†*l*<sub>*s*</sub></sub><sup>*Î±*<sub>*s*</sub></sup>)
for a positive integer *s* such that
$\sum\_{l=1}^{s} \alpha\_{l} \leq 8$ and
*l*<sub>1</sub>â€„â‰ â€„*l*<sub>2</sub>â‰  â‹¯â€„â‰ â€„*l*<sub>*s*</sub>.

**Condition 5:**
âˆ¥*A*<sub>0</sub>*B*âˆ¥<sub>F</sub><sup>2</sup>â€„=â€„*o*(*p*/*n*).

**Condition C1:** There exists a uniformly bounded positive integer
*q*â€„\<â€„*p* such that
$\frac{\sqrt{n} \lambda\_{q}}{\operatorname{tr}(\Sigma)} \rightarrow \infty$
and *Î»*<sub>*q*â€…+â€…1</sub> is uniformly bounded from above. The smallest
eigenvalue *Î»*<sub>*p*</sub> is uniformly bounded from below.

**Condition C2:**
trâ€†(*Î£*<sup>4</sup>)â€„=â€„*o*(tr<sup>2</sup>(*Î£*<sup>2</sup>)).

**Theorem 3**. *Suppose the covariance *Î£* satisfies Condition C1. Under
high-dimensional setting
*n*<sub>1</sub>â€…+â€…*n*<sub>2</sub>â€„=â€„*o*(tr(*Î£*)), Conditions 3,â€†4,â€†5,
and conditions that
*k*<sub>*i*</sub>/*n*<sub>*i*</sub>â€„â†’â€„*Î³*<sub>*i*</sub>â€„âˆˆâ€„(0,1) for
*i*â€„=â€„1,â€†2,
*n*<sub>1</sub>/(*n*<sub>1</sub>+*n*<sub>2</sub>)â€„â†’â€„*Îº*â€„âˆˆâ€„(0,1), then
*U*<sub>LS</sub> has an asymptotically normal distribution with a
uniformly bounded positive *Î»*<sub>0</sub>. More specifically, we have*

*
$$\frac{\lambda\_{0}\left(U\_{\mathrm{LS}}-\mathbb{E}\left(U\_{\mathrm{LS}}\right)\right)}{\sigma\_{n}} \stackrel{d}{\rightarrow} N(0,1), \quad \text { and } \quad \lambda\_{0} \mathbb{E}\left(U\_{\mathrm{LS}}\right)-\left\\W\_{q+1} \mu\_{n}\right\\\_{2}^{2}=o\left(\left\\W\_{q+1} \mu\_{n}\right\\\_{2}^{2}\right),$$
where *q* is the number of divergent eigenvalues of *Î£* as specified in
Condition C1, *W*<sub>*q*â€…+â€…1</sub> is the projection matrix onto the
linear span of eigenspaces of *Î£* corresponding to the smallest
*p*â€…âˆ’â€…*q* eigenvalues,
*Î»*<sub>*q*â€…+â€…1</sub>,â€†*Î»*<sub>*q*â€…+â€…2</sub>,â€†â€¦,â€†*Î»*<sub>*p*</sub>,â€†*Î¼*<sub>*n*</sub>
is the mean difference of the two populations, and
$\sigma\_{n}^{2}=\left(\frac{2}{n\_{1}^{2}}+\frac{2}{n\_{2}^{2}}+\frac{4}{n\_{1} n\_{2}}\right) \sum\_{i=q+1}^{p} \lambda\_{i}^{2}$.
*

**Theorem 4**. *Suppose the covariance *Î£* satisfies Condition C2. Under
high-dimensional setting
*n*<sub>1</sub>â€…+â€…*n*<sub>2</sub>â€„=â€„*O*(tr(*Î£*)), Conditions 3-5, and
conditions that
*k*<sub>*i*</sub>/*n*<sub>*i*</sub>â€„â†’â€„*Î³*<sub>*i*</sub>â€„âˆˆâ€„\[0,â€†1) for
*i*â€„=â€„1,â€†2,
*n*<sub>1</sub>/(*n*<sub>1</sub>+*n*<sub>2</sub>)â€„â†’â€„*Îº*â€„âˆˆâ€„(0,1), and*

*
(*k*<sub>1</sub>+*k*<sub>2</sub>)â€„=â€„*o*(tr(*Î£*)/*Î»*<sub>max</sub>(*Î£*)<sup>2</sup>)
where *Î»*<sub>max</sub>(*Î£*) is the largest eigenvalue of
*Î£*,â€†*Î»*<sub>0</sub>*U*<sub>LS</sub> is asymptotically normally
distributed and has the same asymptotic variance and similar expectation
with *T*<sub>CQ</sub>,*

*
$$\begin{aligned}
& \frac{\lambda\_{0}\left(U\_{\mathrm{LS}}-\mathbb{E} U\_{\mathrm{LS}}\right)}{\sqrt{\operatorname{var} T\_{\mathrm{CQ}}}} \stackrel{d}{\rightarrow} N(0,1), \quad \text { and } \\
& \left\|\mathbb{E}\left(\lambda\_{0} U\_{\mathrm{LS}}-T\_{\mathrm{CQ}}\right)\right\|=o\left(\mathbb{E} T\_{\mathrm{CQ}}\right),
\end{aligned}$$
and *U*<sub>LS</sub> and *T*<sub>CQ</sub> have the same asymptotic
power, *Î²*<sub>*U*<sub>LS</sub></sub>(*Î¼*<sub>1</sub>âˆ’
*Î¼*<sub>2</sub>)â€…âˆ’â€…*Î²*<sub>*T*<sub>CQ</sub></sub>(*Î¼*<sub>1</sub>âˆ’*Î¼*<sub>2</sub>)â€„â†’â€„0.
*

Above *T*<sub>*C**Q*</sub> denote the two-sample statistic in Chen and
Qin (2010) (Chen and Qin 2010). Now it can be observed that for high
correlation covariance *Î£*, Theorem
<a href="#thm3" data-reference-type="ref" data-reference="thm3">3</a>
gives the asymptotic distribution of *U*<sub>*L**S*</sub> and for low
correlation covariance *Î£*, Theorem
<a href="#thm4" data-reference-type="ref" data-reference="thm4">4</a>
does the same. Note that, Theorem
<a href="#thm3" data-reference-type="ref" data-reference="thm3">3</a>
and
<a href="#thm4" data-reference-type="ref" data-reference="thm4">4</a>
provide different formulas to calculate the asymptotic variance under
different structures of covariances. So we need to calculate the
p-values using randomization only. Now when the number of samples
becomes greater i.e., *K*â€„\>â€„2, although the p-values can be obtained
through randomization, there are no proper documentation of the
U-projection statistic. We think this is an area which is still under
research and can be developed further.

# References

Anderson, TW. 2003. â€œAn Introduction to Statistical Multivariate
Analysis.â€ John-Wiley.

Bai, Zhidong, and Hewa Saranadasa. 1996. â€œEffect of High Dimension: By
an Example of a Two Sample Problem.â€ *Statistica Sinica*, 311â€“29.

Chen, Song Xi, and Ying-Li Qin. 2010. â€œA Two-Sample Test for
High-Dimensional Data with Applications to Gene-Set Testing.â€

Muirhead, Robb J. 2009. *Aspects of Multivariate Statistical Theory*.
John Wiley & Sons.

Runze Li, Changcheng Li. 2022. â€œLinear Hypothesis Testing in Linear
Models with High-Dimensional Responses.â€ *Journal of the American
Statistical Association* 117 (540): 1738â€“50.
